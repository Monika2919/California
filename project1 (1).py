# -*- coding: utf-8 -*-
"""Project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e_gb7ugftiA4b70cpWY5l_Ye6CLvX8Yo
"""

#*****************************************************************************
# PROJECT - 1 CALIFORNAIA HOUSING PRICE                    Name : Monika.K
#            [LINEAR REGRESSION AND MULTIPLE REGRESSION]
#****************************************************************************

# Libraries
import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error



#*****************************************************
# Step 1 DATA MANIPULATION
#*****************************************************
# Load DataSet
data=pd.read_csv("/content/california.csv")

# Manipulate Data

#Show First Five Rows
data.head(5)

#Rename Column
data.rename(columns={"Price":"Cost"},inplace=True)
data.head(5)

# Basic Data Functions tail,describe,info,shape,isnull
data.tail()
data.info()
data.shape
data.isnull().sum()
data.drop_duplicates()
data.sort_values("Age",ascending=False)
data.iloc[0:3]
data.describe()

# ***************************************************
# Step 2 - EXPLORATARY DATA ANALYSIS
#****************************************************
# mean,median,minimum,maximum
# The Average value of Population
average_population=data["Population"].mean()
print("Average Population :",average_population)

#  The middle value which helps understand data skewness.
median_rooms=data["Rooms"].median()
print("Median Cost :",median_rooms)

#The range of values shows the spread and possible outliers.
minimum_age=data["Age"].min()
print("Minimum Age :",minimum_age)

maximum_bedrooms=data["Bedrooms"].max()
print("Maximum Bedrooms : ",maximum_bedrooms)

#Indicates how much values vary from the mean.
standard_deviation=data["Cost"].std()
print("Standard Deviation :",standard_deviation)

# visualisation
# histogram chart for cost
sns.histplot(data["Cost"],kde=True,color="green")
plt.xlabel("Cost")
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.title("Histogram Chart For Cost")
plt.show()
# scatter chart Income vs Cost
plt.figure(figsize=(8,5))
sns.scatterplot(x=data["Income"],y=data["Cost"],color="red")
plt.title("\n Scatter Plot For Income vs Cost",fontsize=14, fontweight='bold')
plt.xlabel("Income")
plt.ylabel("Cost")
plt.show()


# heatmap chart of all features
correlation=data[["Income","Age","Rooms","Bedrooms","Occupancy","Latitude","Longitude","Population","Cost"]].corr()
sns.heatmap(correlation,annot=True,cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()
r = data['Income'].corr(data['Cost'])
print("Correlation:", round(r, 2))

# Interpret :
# histogram chart for cost
# The cost data is positively skewed, with most values clustered at lower levels and a few high-cost outliers.
# scatter chart Income vs Cost
# Income and cost are positively related, but the scattered pattern indicates variability and possible outliers
# heatmap chart of all features
# Income is strongly correlated with Cost, Rooms and Bedrooms show high multicollinearity,
# and most other features have weak relationships with Cost.

# *************************************************************************
# Step 3 - PREPROCESSING AND OUTLIER DETECTION
#**************************************************************************
from sklearn.preprocessing import StandardScaler
# Scaling
print("\nOriginal Data\n")
print(data.head(5))
scaler=StandardScaler()
scaled=scaler.fit_transform(data)
print("\nAfter Standard Scaling\n")
print(scaled)

# Detecting outlier
sns.boxplot(data["Income"])
plt.title("Outlier For Income")
plt.show()

sns.boxplot(data["Rooms"])
plt.title("Outlier For Rooms")
plt.show()

# Remove outlier
Q1=data["Income"].quantile(0.25)
Q3=data["Income"].quantile(0.75)
IQR=Q3-Q1
lower=Q1-1.5*IQR
upper=Q3-1.5*IQR

outlier=data[(data["Income"]<lower)|(data["Income"]>upper)]
print(outlier)
clean=data[(data["Income"]>=lower)&(data["Income"]<=upper)]



# After Removing Hide the Outlier For Income
sns.boxplot(data["Income"],showfliers=False)
plt.title("After Removing Outlier (Income)")
plt.show()

Q1=data["Rooms"].quantile(0.25)
Q3=data["Rooms"].quantile(0.75)
IQR=Q3-Q1
lower1=Q1-1.5*IQR
upper1=Q3-1.5*IQR
outlier_r=data[(data["Rooms"]<lower1)|(data["Rooms"]>upper1)]
print(outlier_r)
clean1=data[(data["Rooms"]>=lower1)&(data["Rooms"]<=upper1)]
print(clean1)
# After Removing Hide the Outlier For Rooms
sns.boxplot(data["Rooms"],showfliers=False)
plt.title("After Removing Outlier (Rooms)")
plt.show()

#***********************************************************
# Step - 4 TRAIN TEST AND SPLIT
#***********************************************************
x = data.drop("Cost",axis=1)
y = data["Cost"]
print("Original Dataset:\n")
print(data["Cost"].value_counts())
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=50)
print("Normal Train\n")
print(y_train.value_counts())
print(" Normal Test\n")
print(y_test.value_counts())

# Stratified split
# We will create 5 income categories for stratification.
income_bins = pd.cut(data["Income"], bins=5, labels=False)
data["Income_Groups"] = income_bins


# Perform stratified split using the newly created 'Income Groups'
x_train_strat, x_test_strat, y_train_strat, y_test_strat = train_test_split(x, y,test_size=0.20, random_state=50,stratify=data["Income_Groups"])
print("Stratified Train\n")
print(y_train_strat.value_counts())
print("Stratified Test\n")
print(y_test_strat.value_counts())
# Overall distribution
overall_dist = data["Income_Groups"].value_counts().sort_index()
print("Overall Distribution:\n",overall_dist)

# Compare Distributions
train_dist = data.loc[x_train_strat.index, "Income_Groups"].value_counts().sort_index()
test_dist = data.loc[x_test_strat.index, "Income_Groups"].value_counts().sort_index()
print("Train Income Group Distribution:\n",train_dist)
print("Test Income Group Distribution:\n",test_dist)

# To verify stratification, we can compare the distributions
print("\nComparison of Income Group Distributions:")
comparison_df = pd.DataFrame({'Overall': overall_dist, 'Train': train_dist, 'Test': test_dist})
print(comparison_df)

#**********************************************
# Step - 5 REGRESSION MODEL
#*********************************************
# LINEAR REGRESSION
x=data[["Income"]]
y=data["Cost"]
model_li=LinearRegression()
model_li.fit(x,y)
data["Predicted_cost"]=model_li.predict(x)
print("Predicted Cost : \n",model_li.predict(np.array([[5]])))
print("Actual Cost\n:",data["Cost"])

# Calculate Accuracy For Linear Regression
actual = data["Cost"]
predicted = data["Predicted_cost"]
rs=r2_score(actual,predicted)
print( "R-squared (Linear Regression):",rs)


# Multiple Regression
x_m=data[["Income","Age","Rooms","Bedrooms","Population","Occupancy","Latitude","Longitude"]]
y_m=data["Cost"]
model_m=LinearRegression()
model_m.fit(x_m,y_m)

data["Predicted_cost"]=model_m.predict(x_m)
print("Acutal Cost:\n",data["Cost"].head())
print("Predicted Cost:\n",model_m.predict(x_m.head(5)))

# Calculate Accuracy For Multiple Regression
actual_m = data["Cost"]
predicted_m = data["Predicted_cost"]
rs_m=r2_score(actual_m,predicted_m)
print( "R-squared (Multiple Regression):",rs_m)

# Compare Accuracy For LinearRegression and MultipleRegression
plt.figure(figsize=(4,5))
plt.bar(["Linear Regression","Multiple Regression"],[rs,rs_m])
plt.ylabel("R-squared")
plt.title("Comparison of R-squared for Linear and Multiple Regression\n",fontweight="bold")
plt.show()

# *************************************************************************
# STEP - 6 CHECK ASSUMPTIONS
#*************************************************************************
# Linear Regression
# Take Sample Data For Assumptions
import scipy.stats as stats
ass=data.sample(20,random_state=20)
ass.to_csv("/content/assumption.csv",index=False)
x=ass[["Rooms"]]
y=ass["Cost"]
model_li=LinearRegression()
model_li.fit(x,y)
ass["Predicted_cost"]=model_li.predict(x)
ass["Residual"]=y-ass["Predicted_cost"]
residual=ass["Residual"]



# linearty check for linear regression

plt.scatter(ass["Rooms"],ass["Cost"],color="green",label="Actual")
plt.plot(ass["Rooms"],ass["Predicted_cost"],color="red",label="Predicted")
plt.xlabel("Income")
plt.ylabel("Cost")
plt.title("Linearity Check")
plt.legend()
plt.show()


# independence
plt.plot(ass["Residual"].reset_index(drop=True),marker='*',linestyle="")
plt.axhline(0,color="red",linestyle='--')
plt.xlabel("Observation order")
plt.ylabel("Residuals")
plt.title("Independence Check")
plt.show()

# homoscedasicity
plt.figure(figsize=(10,5))
plt.scatter(ass["Predicted_cost"],ass["Residual"],color="purple")
plt.axhline(0,color="orange",linestyle='--')
plt.xlabel("Predicted Cost")
plt.ylabel("Residuals")
plt.title("Homoscedasiticity Check")
plt.show()

#normality of residual
sns.histplot(ass["Residual"],kde=True,color="brown")
plt.title("Normality Check")
plt.show()


# Q-Q plot
plt.figure(figsize=(6,4))
stats.probplot(residual, plot=plt)
plt.title("Q–Q Plot of Residuals", fontweight="bold")
plt.show()

# Multiple Regression
# Take Sample Data For Assumptions
ass1= data.sample(20,random_state=20)
ass1.to_csv("/content/assumption1.csv",index=False)

#fit multiple regression
x=ass1.drop("Cost",axis=1)
y=ass1["Cost"]
model_m=LinearRegression()
model_m.fit(x,y)
ass1["Predicted_cost"]=model_m.predict(x)
ass1["Residual"]=y-ass1["Predicted_cost"]

# linearity check
plt.scatter(ass1["Rooms"],ass1["Cost"],color="green",label="Actual")
plt.plot(ass1["Rooms"],ass1["Predicted_cost"],color="red",label="Predicted")
plt.xlabel("Income")
plt.show()

# independance
plt.plot(ass1["Residual"].reset_index(drop=True),marker='*',linestyle="")
plt.axhline(0,color="red",linestyle='--')
plt.xlabel("Observation order")
plt.ylabel("Residuals")
plt.title("Independence Check")
plt.show()

#homoscedasticity check
plt.figure(figsize=(10,5))
plt.scatter(ass1["Predicted_cost"],ass1["Residual"],color="purple")
plt.axhline(0,color="orange",linestyle='--')
plt.xlabel("Predicted Cost")
plt.ylabel("Residuals")
plt.title("Homoscedasiticity Check")
plt.show()

#normality of residuals
sns.histplot(ass1["Residual"],kde=True,color="brown")
plt.title("Normality Check")
plt.show()

# Q-Q plot
plt.figure(figsize=(6,4))
stats.probplot(ass1["Residual"], plot=plt)
plt.title("Q–Q Plot of Residuals", fontweight="bold")
plt.show()

# ****************************************************
# Step 7: Residuals & Error Metrics
# ****************************************************
# AE,MSE,RMSE,R2

# Linear Regression
x_li = data[["Income"]]
y_li = data["Cost"]
model_li = LinearRegression()
model_li.fit(x_li, y_li)

actual = data["Cost"]
predicted = model_li.predict(x_li) # Use model_li.predict(x_li) for consistency
# AE,MSE,RMSE,R2 for linear regression
ae=mean_absolute_error(actual,predicted)
mse=mean_squared_error(actual,predicted)
rmse=math.sqrt(mse)
rs=r2_score(actual,predicted)

print(f"MAE: {ae:.2f}")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R-squared: {rs:.2f}")


# AE,MSE,RMSE,R2
# Multiple Regression

x_m = data[["Income","Age","Rooms","Bedrooms","Population","Occupancy","Latitude","Longitude"]]
y_m = data["Cost"]
model_m = LinearRegression()
model_m.fit(x_m, y_m)

actual_m = data["Cost"]
predicted_m = model_m.predict(x_m)

# AE,MSE,RMSE,R2 for linear regression
ae_m=mean_absolute_error(actual_m,predicted_m)
mse_m=mean_squared_error(actual_m,predicted_m)
rmse_m=math.sqrt(mse_m)
rs_m=r2_score(actual_m,predicted_m)

print(f"MAE: {ae_m:.2f}")
print(f"MSE: {mse_m:.2f}")
print(f"RMSE: {rmse_m:.2f}")
print(f"R-squared: {rs_m:.2f}")

# Interpret Results
# LinearRegression
# “The model’s predictions are, on average, 0.63 units away from the actual values, with an RMSE of 0.84.
# The model explains 47% of the variation in the target variable, indicating moderate predictive performance.”
# MultipleRegression
# The model’s predictions are on average 0.53 units off (RMSE = 0.72), and it explains
# 61% of the variation in the target variable.

#******************************************************************
# STEP - 8 : FINAL REPORT
#*****************************************************************
# 1. Which variables impact house price the most?
# Median Income, Average Bedrooms, Latitude, and Longitude impact house prices the most.
# Population and Average Occupancy have very little impact.

# 2️ Did assumptions hold true?
# Most assumptions were reasonably satisfied. Linearity and independence were met,
# and residuals were approximately normal due to the large sample size. Minor violations are acceptable in real-world data.

# 3. Which model was better?
# Multiple Linear Regression was better

# 4. Are errors acceptable?
# The errors are partially acceptable, and the multiple regression model is clearly better than the linear regression model.

# Final Conclusion
# The multiple regression model performs well, assumptions are mostly satisfied, and prediction errors are acceptable.











